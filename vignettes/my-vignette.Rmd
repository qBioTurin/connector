---
title: "An Introduction to `connector`"
author: "Simone Pernice, Roberta Sirovich and Francesca Cordero"

header-includes:
   - \usepackage{amsfonts,amsmath}
output: 
  pdf_document:
    toc: true
    toc_depth: 3
    fig_caption: yes
    
bibliography: biblio.bib  
vignette: >
  %\VignetteIndexEntry{my-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "##",
  results = TRUE,
  warning = FALSE,
  message = FALSE,
  highlight = TRUE,
  background = 'grey',
  fig.cap = TRUE
)
rm(list=ls())
```
# Introduction

Connector is built on the model-based approach for clustering functional data presented in [@james2003clustering]. Such method is particularly effective when the observations are sparse and irregularly spaced, as growth curves usually are. A review on the method and on the tools developed by the authors to suitably set the free parameters, please refer to [our NM paper].

[qui magari due parole sulla rilevanza biologica?] 

The method models individual curves $g_i$ using basis functions
\begin{equation}\label{eq:gspline}
	g_i(t) = \mathbf{s}(t)^T  \boldsymbol{\eta}_i,
\end{equation}
where $\mathbf{s}(t)$ is a $p-$dimensional spline basis vector and $\boldsymbol{\eta}_i$ is a vector of spline coefficients. The $\boldsymbol{\eta}_i$'s are treated with a random-effects model rather than considering them as parameters and fitting a separate spline curve for each individual. Moreover, a suitable parametrization of the mean curves allows a lower-dimensional representation of the curves with means in a restricted subspace. The model is fitted through an EM procedure.

Particular attention will be devoted to model selection. In particular, one must choose the number of clusters to fit, the dimension of the spline basis and the reduced dimension of the mean space. `connector` provides a complete toolkit to lead the user through such decisions, one of the most difficult problems in cluster analysis.




# Installation

The `connector` package is hosted on the GitHub platform. The simplest way to obtain `connector` is to install it using `devtools`. Type the following commands in R console:  

```{r, eval=FALSE}

# Install
install.packages("devtools",repos = "http://cran.us.r-project.org")
library(devtools)
install_github("qBioTurin/connector", ref="master",dependencies=TRUE)
# Load
library(connector)

```

Users may change the `repos` options depending on their locations and preferences. For more details, see `help(install.packages)`.  

```{r, echo=FALSE, results=FALSE}

library(connector)

```

# Quick Start

A demo is available to provide the list of commands to perform the analysis of one of the datasets considered in [ref: connector paper]. To run the example just type:  

```{r, eval=FALSE}

demo("MainCommandsList", package = "connector")

```

# Use case: tumor growth dataset

We illustrate the functionalities and the usage of the `connector` package on a tumor growth curves dataset. It is included in the package's data and it is organized as any observed curves are expected by the package's functions.

Observations should be saved into two distinct files:

1. an excel file reporting the discretely sampled curve data. As functional data are longitudinal, we respect the convention of parametrizing the models in terms of time $t$ and y-values $y$. Each sample is represented as two columns of the excel file, the first one named *time* includes the lags list and the second one named with the *sample name* (each sample has different ID name, for example 475_P1b is the ID name of the first sample in the following figure) includes the list of observed $y$ values. Hence, if we record 24 tumor growth curves, the file have 48 columns. See Figure \ref{fig:xlsx}.

1. a csv file containing the annotations associated to the sampled curves. It is composed by a number of rows equal to the number of samples. The first row gives the columns names as follows: the first twos must be a numerical ID and the corresponding ID name *sample name*. The remaining ones codify for the annotated features. Notice that the column *sample name* should contain the same ID names which appear in the sampled curve excel file. See Figure \ref{fig:csv}.


![First lines of the excel file of the sampled curve data.\label{fig:xlsx}](./Fig/excel_file.png){ width=90% }

![First lines of the csv file of the annotated features.\label{fig:csv}](./Fig/txt_file.png)

## Data Importing

Once data have been prepared in the two files above described, they are imported by the `DataImport` function. Two arguments, with the file names respectively, should be specified. In this example the full path names have been saved in the `GrwoDatFile` and `AnnotationFile` strings.

```{r}

# find the full path names of the example files
GrowDataFile<-system.file("data", "475dataset.xlsx", package = "connector")
AnnotationFile <-system.file("data", "475info.txt", package = "connector")
# import the samples
CONNECTORList<-DataImport(GrowDataFile = GrowDataFile,
                          AnnotationFile = AnnotationFile)

```

A list of four object is created:

```{r}

# show the CONNECTORList structure
str(CONNECTORList)

```

The components of the `CONNECTORList` are:

1. `$Dataset`, a data frame with three variables: `ID` of the curve, `Vol` the $y$ values and `Time` the time lags;

2. `$LenCurv`, the vector of the number of observations per sample;

3. `$LabCurv`, a data frame matching the sample with the corresponding annotated features. Hence the variables are extracted and named from the `AnnotationFile`;

4. `$TimeGrid`, the vector of the complete time grid points.

### Data importing by data.frame
 
Instead of using two files, two data frames can be exploited to generate `CONNECTORList`. Specifically

1. *GrowDataFrame*: dataframe of three columns storing the time series. The first columns, called "\emph{ID}", stores the identification number of each sample. The second column, labeled "\emph{Vol}", contains  the data volume over the time and the respective time point is reported in the third column, labeled  "\emph{Time}";

2. *AnnotationFrame*: dataframe with a number of rows equal to the number of samples in the *GrowDataFrame*. The first column must be named "\emph{ID}" and it stores the identification numbers exploited for the matching with the samples saved in the *GrowDataFrame*. Then it is possible to add a column per feature to consider and to associate with the respective sample (depending on the identification number in the first column). If NULL, then in the `CONNECTORList` only the feature \emph{ID} will be reported.

Hence, `CONNECTORList` can be generated by exploting the *DataFrameImport* function.

```{r,eval=F}

# show the CONNECTORList structure
CONNECTORList <- DataFrameImport(GrowDataFrame = GrowDataFrame,
                                 AnnotationFrame = AnnotationFrame)
                                

```


## Data Visualization 

The `GrowthCurve` function provides a plot of the sampled curves coloured by a user selected feature out of the ones given in the `AnnotationFile`. In Figure \ref{fig:growthcurves} an example is illustrated. It has been produced by the following code:

```{r, results="hide", fig.cap="Sampled curves, coloured by progeny feature. \\label{fig:growthcurves}"}

GrowPlot<-GrowthCurve(data = CONNECTORList,
                      feature = "Progeny")
GrowPlot

```

The `DataVisualization` function plots the sampled curves and the density of the time grid. This latter plot may be used to verify the density of the collected observations and eventually decide for a truncation.

```{r, fig.height = 3, fig.width = 7, fig.align = "center", fig.cap="Sampled curves, coloured by progeny feature (left panel) and time grid density (right panel). \\label{fig:gridDensity}"}

# Growth curves and time grid visualization 
Datavisual<-DataVisualization(data = CONNECTORList,
                              feature = "Progeny", 
                              labels = c("Time","Volume","Tumor Growth"))
Datavisual

```

```{r, echo=F}

tronca = 70

```

Curves may be (and it is often the case) irregularly and sparsely sampled. As a preliminary step, it may be useful to check whether the assembled pairs $(t_{i_j},t_{i_k})$, for alla subjects $i=1, \dots, n$ and lags $i_1,\dots,i_{n_i}$, are sufficiently dense in the domain plane. Low-density subregions may be excluded from the analysis and hence a truncation time for the observations imposed. The function `DataTruncation` have been developed to this aim. According to Figure \ref{fig:gridDensity}, we decide to truncate the observations at `r tronca` days.

```{r}

# data truncation
trCONNECTORList<-DataTruncation(data = CONNECTORList,
                              feature="Progeny",
                              truncTime = 70,
                              labels = c("Time","Volume","Tumor Growth"))
# the trCONNECTORList structure
str(trCONNECTORList, max.level = 1)

```

The output of the function `DataTruncation` is a list of six objects. The firt four of which are the truncated versions of the `DataImport` function. The plot stored in `$GrowthCurve_plot` shows the sampled curves plot with a vertical line at the truncation time, see Figure \ref{fig:truncation}.

```{r, fig.cap="Sampled curves, coloured by progeny feature and truncation lag (vertical solid black line). \\label{fig:truncation}"}

# plot
trCONNECTORList$GrowthCurve_plot

```


## Model Selection Tools

Several model selection questions need a tool set to be addressed. In particular the free parameters to be set are:

1. the spline basis dimension, $p$;

2. the number of clusters, $G$.

We developed a tool to help the user define each of the above parameters. Let us stress out that rough choices for the free parameters may compromise the full analysis. 

### The spline basis dimension

As proposed in [@james2000principal], the dimension of the spline basis $p$ can be taken to corresponding to the largest cross-validated likelihood. The function `BasisDimension.Choice` performs a ten-fold crossvalidation for each of the values of `p` decided by the user. In details two plots are returned:

1. The `$CrossLogLikePlot` gives a visual representation of the results, see Figure \ref{fig:crossloglike}. Each gray dashed line corresponds to the crossloglkelihood values obtained on different test/learning sets and the solid black line is their mean value. The user should choose the the smallest value of `p` that ensures larger values of the mean crossloglikelihood function. **It may be useful to keep in mind that working with sparse data encourage to spare parameters and hence smaller values of $p$ are preferable. LA CANCELLIAMO QUESTA FRASE??**

2. The `$KnotsPlot` shows the growth curves (upper plot) together with the knots distribution over the time grid (lower plot). Specifically, the lower plot is characterized by one horizontal line for each $p$ value passed in input to the function, and following this line it is showed how the knots defining the spline of dimension $p$ are distributed over the time grid defined by the input curves. The user should choose $p$ such that the number of cubic polynomials (i.e., $p-1$) defined in each interval is able to follow the curves dynamics.


```{r, fig.height = 3, fig.width = 7, fig.align = "center",fig.cap="Cross-validated loglikelihood functions. \\label{fig:crossloglike}"}

# ten-fold crossvalidation 
CrossLogLike<-BasisDimension.Choice(data = trCONNECTORList,
                                    p = 2:6 )
CrossLogLike$CrossLogLikePlot

```

```{r, fig.height = 8, fig.width = 7, fig.align = "center",fig.cap="Knots ditribution. \\label{fig:Knotscrossloglike}"}

CrossLogLike$KnotsPlot
# set p
p <- 3

```

Hence, in the current example, we set $p=3$, as it is the smallest value that ensures a large loglikelihood.


### The number of clusters

Setting properly the number of clusters to fit is a well known problem in cluster analysis. `connector` provides two different plots to guide the user. 

In [qui rif al nostro paper] we introduced two measures of proximity, the *total tightness* $T$ and the *functional Davied-Bouldin index* fDB. Both measures rely on on the family of semi-metrics between curves defined as
\begin{equation}\label{eq:qsemimetrics}
	D_q(f,g) = \sqrt{ \int \left| f^{(q)}(s)-g^{(q)}(s) \right|^2 ds }, \qquad d=0,1,2,
\end{equation} 
where $f$ and $g$ are two curves and $f^{(q)}$ and $g^{(q)}$ are their $q$th derivatives. 

The *total tightness* $T$ is the dispersion measure defined as
\begin{equation}\label{eq:totalT}
	 T = \sum_{k=1}^G \sum_{i=1}^n D_0(\hat{g}_i, \bar{g}^k),
\end{equation}
where $\hat{g}_i$ is the estimated $i$--th curve given in eq. \eqref{eq:gsplinepred} and $\bar{g}^k$ is the center of $k$--th cluster given in eq. \eqref{eq:splinemeancurve}. As the number of clusters increases, the total tightness decreases to zero, the value which is attained when the number of fitted clusters equals the number of sampled curves. In this limiting case, any $k$th cluster mean curve coincides with an estimated curve and $D_0(\hat{g}_i, \bar{g}^k) = 0$ for any $i$ and $k$. A proper number of clusters can be inferred as large enough to let the total tightness drop down to relatively little values but as the smallest over which the total tightness does not decrease substantially. Hence, we look for the location of an ``elbow" in the plot of the total tightness against the number of clusters. 


In [qui rif al nostro paper] we defined a second index, which is a cluster separation measure, and we called it *functional* DB (fDB). It is defined as follows
\begin{equation}\label{eq:fDB}
	\mbox{fDB}_q = \frac{1}{G} \sum_{k=1}^G \max_{h \neq k} \left\{  \frac{S_h + S_k}{M_{hk}}  \right\},
\end{equation} 
where, for each cluster $k$ and $h$ 
\begin{equation*}
	S_k = \sqrt{\frac{1}{G_k} \sum_{i=1}^{G_k} D_q^2(\hat{g}_i, \bar{g}^k)} \qquad \mbox{and} \qquad  M_{hk} =  D_q(\bar{g}^h, \bar{g}^k),
\end{equation*}
with $G_k$ the number of curves in the $k$th cluster. The significance of eq. \eqref{eq:fDB} can be understood as the average of the blend measures of each cluster from its most overlapping cluster. The ``best" choice of clusters, then, will be that which minimizes this average blend. 

Furthemore, given the random initialization of the k-means algorithm to get initial cluster memberships into the FCM algorithm and the stochasticity characterizing the method lead to high variability among the runs. For these reasons, multiple runs are necessary to identify the most frequent clustering fixed a number of clusters ($G$).

To effectively take advantage of those two measures, `connector` supplies the function `ClusterAnalysis` which repeats the clustering procedure a number of times equal to the parameter `runs` and for each of the number of clusters given in the parameter `G`. The output of the function is a list of three objects,

1. `$Clusters.List`: the list of all the clustering divisions obtained varying among the input $G$ values;

2. `$seed`: the seed sets before running the method;

3. `$runs`: the number of runs.

Specifically, the object storing the clustering obtained with $G=2$ (i.e., `ClusteringList$Clusters.List$G2`) is a list of three elements:

1. `$ClusterAll`: the list of all the possible FCM parameters values (see Sec. *Details on the functional clustering model*) that can be obtained through each run. In details, the item `$ParamConfig.Freq` reports the number of times that the respectively parameters configuration (stored in `$FCM`) is found. Let us note thaty the sum might be not equal to the number of runs since errors may occur;

2. `$ErrorConfigurationFit`: list of errors that could be obtained by running the FCM method with extreme parameters configurations;

3. `$h.selected`: the $h$ value selected to perform the analysis. In details, this parameter gives a further parametrization of the mean curves allowing a lower-dimensional representation of the curves with means in a restricted subspace. Its value is choosen such that the inequality $h \le \min(G-1,\ p)$ holds. Better clusterings are obtained with higher values of $h$, but this may lead to many failing runs. In this cases $h$ values is decreased until the number of successful runs are higher than a specific constraint which can be defined by the user ( by default is $100\%$ of successful runs).


```{r,message=FALSE, eval=FALSE}

ClusteringList <-ClusterAnalysis(data = trCONNECTORList,
                                 G = 2:5,
                                 p = p,
                                 runs = 100)

```

```{r,message=FALSE,echo=F}
ClusFile <- "Clustering.RData"
if(!file.exists(ClusFile))
{
ClusteringList <-ClusterAnalysis(data = trCONNECTORList,
                                 G = 2:5,
                                 p = p,
                                 runs = 100)
  
  save(ClusteringList, file =ClusFile)
}else{
  load(ClusFile)
}
G=3
```

```{r}
# the output structure
str(ClusteringList, max.level = 2, vec.len=1)
str(ClusteringList$Clusters.List$G2, max.level = 3, vec.len=1)
```



By means of the function `IndexesPlot.Extrapolation` the two plots represented in Figure \ref{fig:TandfDB} can be generated from `ClusteringList`. Indetails, the tightness and fDB indexes are plotted for each value of $G$. The vertical lines associated with each $G$ are violin plots in which the points represent the index values for the respectively runs. The blu line shows the most probable configuration
which will be exploited hereafter.


```{r, fig.height = 7, fig.width = 16, fig.align = "center",fig.cap="\\label{fig:TandfDB} Violin Plots of the {\\it total tightness} $T$ calculated on each run and for different number of clusters $G$ (right panel). Violin Plots of the {\\it functional DB index} fDB calculated on each run and for different number of clusters $G$ (left panel)."}

IndexesPlot.ExtrapolationNew(stability.list =ClusteringList )

```

The indexes show that $G=3$ and $G=4$ may be good choices for the parameter. Specifically, the left panel shows that $G=3$ may be good choice for the parameter, while the fDB indexes plotted in the left panel lead the choice to $G=4$, a number of clusters that explicitly minimizes the fDB index.

The variability of the two measures among runs, exhibited in Figure \ref{fig:TandfDB}, is related to the random initialization of the k-means algorithm to get initial cluster memberships from points. The stability of the clustering procedure can be visualized through the consensus matrix extrapolated by the function `ConsMatrix.ExtrapolationNew`, as shown in Figure \ref{fig:ConsMatg4}.
Let us note that the number associated with each cluster represents the average number of time that the curves belonging to the same cluster are counted in the most probable cluster; whilethe average of these indexes is reported in the title.

```{r, fig.height =4, fig.width = 6, fig.align = "center",fig.cap="\\label{fig:ConsMatg4} Consensus Matrices for G = 3 and 4. "}

ConsMatrix<-ConsMatrix.ExtrapolationNew(stability.list = ClusteringList,
                                        data = trCONNECTORList)
str(ConsMatrix, max.level = 2, vec.len=1)

ConsMatrix$G3$ConsensusPlot
ConsMatrix$G4$ConsensusPlot

```

Hence, for the illustrated example, we are choosing $G=3$.

Once the free parameters are all set, the function `MostProbableClustering.Extrapolation` can be used to fix the most probable clustering with given dimension of the spline basis $p$, and number of clusters $G$ and save the result in a dedicated object.

```{r, fig.height = 6, fig.width = 6, fig.align = "center", out.width= '90%'}

CONNECTORList.FCM.opt<-MostProbableClustering.ExtrapolationNew(
                                                  stability.list = ClusteringList,
                                                  G = G )
```

## Results Visualization and Inspection

`connector` provides the function `ClusterWithMeanCurve` which plots the sampled curves grouped by cluster membership, together with the mean curve for each cluster, see Figure \ref{fig:clusters}. The function prints as well the values for $S_h$, $M_{hk}$ and fDB given in equation \eqref{eq:fDB}.

```{r, fig.height = 10, fig.width = 15, fig.align = "center", fig.cap="\\label{fig:clusters} Sampled curves grouped by cluster membership."}

FCMplots<- ClusterWithMeanCurve(clusterdata = CONNECTORList.FCM.opt,
                                data = trCONNECTORList,
                                feature = "Progeny",
                                labels = c("Time","Volume"),
                                title = "FCM model")

```

A detailed visualization of the clusterization of the sample curves can be obtained by means of the `DiscriminantPlot` function. In [@james2003clustering], the authors describe how to obtain low-dimensional plots of curve datasets, enabling a visual assessment of clustering. They propose to project the curves into the lower dimensional space of the mean space, so that they can be plotted as points (with coordinates the functional linear discriminant components), making it much easier to detect the presence of clusters. Moreover, in case $h=2$ and hence two functional linear discriminant components are calculated. In the case study here described, we get the plots in Figure \ref{fig:DiscrPlotCL} which is colored by cluster membership and in Figure \ref{fig:DiscrPlotF} which is colored by the user selected feature called `"Progeny"`.


```{r, fig.height = 4, fig.width = 6, fig.align = "center",fig.cap=c("\\label{fig:DiscrPlotCL} Curves projected onto the 2-dimensional mean space: the functional linear discriminant $\\alpha_1$ versus the functional linear discriminant $\\alpha_2$. Symbols are coloured by cluster membership.", "\\label{fig:DiscrPlotF} Curves projected onto the 2-dimensional mean space: the functional linear discriminant $\\alpha_1$ versus the functional linear discriminant $\\alpha_2$. Symbols are coloured by progeny.") }

DiscrPlt<-DiscriminantPlot(clusterdata = CONNECTORList.FCM.opt,
                           data = trCONNECTORList,
                           feature = "Progeny")

DiscrPlt$ColCluster

DiscrPlt$ColFeature
```

In the end, to inspect the composition of the clusters, the function `CountingSamples` reports the number and the name of samples in each cluster according to the feature selected by the user.

```{r, fig.height = 4, fig.width = 6, fig.align = "center"}

NumberSamples<-CountingSamples(clusterdata = CONNECTORList.FCM.opt,
                               data = trCONNECTORList,
                               feature = "Progeny")

str(NumberSamples, max.level = 2)

```

# Advanced Topics

In [@james2003clustering] the authors suggest to consider all the information about the traits that distinguish one cluster from another. In particular, they calculate the optimal weights to apply to each dimension for determining cluster membership, as shown in Figure \ref{fig:discrimination}. 

```{r, fig.height = 4, fig.width = 6, fig.align = "center", fig.cap="\\label{fig:discrimination} Discriminant curve."}

MaxDiscrPlots<-MaximumDiscriminationFunction(clusterdata = CONNECTORList.FCM.opt)

MaxDiscrPlots[[1]]

```
Large absolute values correspond to large weights and hence large discrimination between clusters. Roughly speaking Figure \ref{fig:discrimination} tells us that earlier measurements are important in determining cluster assignment as well as latter ones.


# Details on the functional clustering model 

The curves, $g_i(t)$ for each $i$th selected individual, are supposed to be observed with measurement errors and only at few discrete time points. Hence the vector $\mathbf{Y}_i$ of observed values at times $t_{i_1}, \dots , t_{i_{n_i}}$ is given as
\begin{equation*}
	\mathbf{Y}_i = \mathbf{g}_i + \boldsymbol{\varepsilon}_i,
\end{equation*}
where $\mathbf{g}_i$  and $\boldsymbol{\varepsilon}_i$ are the vectors of true values and measurement errors at time grid, respectively. As there are only finite number of observations, individual curves are modeled using basis functions, in particular cubic splines. Let
\begin{equation}\label{eq:gspline}
	g_i(t) = \mathbf{s}(t)^T  \boldsymbol{\eta}_i,
\end{equation}
where $\mathbf{s}(t)$ is a $p-$dimensional spline basis vector and $\boldsymbol{\eta}_i$ is a vector of spline coefficients. The $\boldsymbol{\eta}_i$'s are treated with a random-effects model rather than considering them as parameters and fitting a separate spline curve for each individual. Cluster means are furthermore rewritten as
\begin{equation*}
	\boldsymbol{\mu}_{k} = \boldsymbol{\lambda}_0 + \Lambda \boldsymbol{\alpha}_k,
\end{equation*}
where $\boldsymbol{\lambda}_0$ and $\boldsymbol{\alpha}_k$ are $p-$ and $h-$ dimensional vectors, $\Lambda$ is a $(p,h)$ matrix and $h \leq \min(p,G-1)$, where $G$ denote the true number of clusters. This parametrization allows a lower-dimensional representation of the curves with means in a restricted subspace (for $h < G-1$).

With this formulation, the functional clustering model can be written as
\begin{eqnarray}
	\mathbf{Y}_i =S_i  \cdot ( \boldsymbol{\lambda}_0 + \Lambda \boldsymbol{\alpha}_{\mathbf{z}_i} +  \boldsymbol{\gamma}_i) +  \boldsymbol{\varepsilon}_i, \quad i=1, \dots, n,\nonumber\\
	\boldsymbol{\varepsilon}_i \sim  \mathcal{N} (\mathbf{0},R), \quad  \boldsymbol{\gamma}_i \sim   \mathcal{N} (\mathbf{0},\Gamma), \qquad \qquad
\end{eqnarray}
where $S_i = (\mathbf{s}(t_{i_1}),\dots,\mathbf{s}(t_{i_{n_i}}))^T$ is the spline basis matrix for the $i-$th curve. 

The model is fitted following [@james2003clustering] and all the estimated parameters and the predicted cluster membership are returned. Notice that the $k$th cluster mean curve can be retrieved as
\begin{equation}\label{eq:splinemeancurve}
	\bar{g}^k(t) = \mathbf{s}(t)^T ( \hat{\boldsymbol{\lambda}}_0 + \hat{\Lambda} \hat{ \boldsymbol{\alpha}}_k).
\end{equation}
Moreover, the functional clustering procedure can accurately predict unobserved portions of the curves $g_i(t)$ by means of the natural estimate
\begin{equation}\label{eq:gsplinepred}
	\hat{g}_i(t) =  \mathbf{s}(t)^T  \hat{\boldsymbol{\eta}}_i,
\end{equation}
where $\hat{\boldsymbol{\eta}}_i$ is a prediction for $\boldsymbol{\eta}_i$ which is proven to be optimally computed as $\mathbb{E}( \boldsymbol{\eta}_i \;|\; \mathbf{Y}_i)$ and explicitly given in [@james2003clustering], eq. (17). 

# References

---
nocite: |
  @ClassicalModels
...